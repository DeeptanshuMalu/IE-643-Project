{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ast\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context_before', 'equation', 'context_after', 'spoken_English'],\n",
       "    num_rows: 4753354\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = load_from_disk(\"mathbridge_filtered\")\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context_before', 'equation', 'context_after', 'spoken_English'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_1000 = ds_train.shuffle(seed=42).select(range(10**3))\n",
    "ds_train_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\deept\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"Hyeonsieun/MathSpeech_T5_base_translator\", max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def get_openai_response(prompt, transcription):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are a helpful assistant. Your task is to correct the input LaTeX code to make it valid and compilable.\n",
    "                            The input may contain both mathematical and non-mathematical text. Please ensure the output is corrected for both types\n",
    "                            and that all elements are formatted correctly in LaTeX. Return only the corrected LaTeX code and nothing else.\n",
    "                            Do not include any extra commands such as documentclass, begin, or end document. Exclude all additional comments, \n",
    "                            explanations, and any other text. The original transcription is: \"\"\"\n",
    "                + transcription,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for ex in tqdm(ds_train_1000):\n",
    "    transcription = ex[\"spoken_English\"]\n",
    "    prompt = pipe(transcription)[0][\"generated_text\"]\n",
    "    response = get_openai_response(prompt, transcription)\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"openai_responses.txt\", \"w\") as f:\n",
    "    for response in responses:\n",
    "        f.write(repr(response) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"openai_responses.txt\", \"r\") as f:\n",
    "    responses2 = [ast.literal_eval(line) for line in f]\n",
    "len(responses2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TeXBLEU.new_metric import texbleu\n",
    "\n",
    "scores_texbleu = []\n",
    "for row, response in tqdm(zip(ds_train_1000, responses2), total=len(ds_train_1000)):\n",
    "    score_texbleu = texbleu(response, row[\"equation\"])\n",
    "    scores_texbleu.append(score_texbleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8465212999999994"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_texbleu = sum(scores_texbleu) / len(scores_texbleu)\n",
    "final_texbleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.5373747199230257\n",
      "ROUGE-1 score: 0.8528077477496061\n",
      "CER score: 0.5142519219396806\n",
      "WER score: 0.9194364161849711\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the evaluation metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "cer = evaluate.load(\"cer\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "# Prepare references and predictions\n",
    "references = [ex[\"equation\"] for ex in ds_train_1000]\n",
    "predictions = responses2\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score[\"bleu\"])\n",
    "\n",
    "# Calculate ROUGE-1 score\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\"])\n",
    "print(\"ROUGE-1 score:\", rouge_score[\"rouge1\"])\n",
    "\n",
    "# Calculate CER score\n",
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "print(\"CER score:\", cer_score)\n",
    "\n",
    "# Calculate WER score\n",
    "wer_score = wer.compute(predictions=predictions, references=references)\n",
    "print(\"WER score:\", wer_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
