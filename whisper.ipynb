{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import numpy as np\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe_l = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-medium\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe_m = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-small\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe_s = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n"
     ]
    }
   ],
   "source": [
    "fs = 16000\n",
    "\n",
    "# Duration of recording in seconds\n",
    "duration = 60\n",
    "\n",
    "print(\"Recording...\")\n",
    "# Record audio\n",
    "recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=\"int16\")\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording finished.\")\n",
    "\n",
    "# Save the recording as a WAV file\n",
    "write(\"output.wav\", fs, recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was 1, as k goes to infinity. Now, it's the last time this is what we did, and I just wanted to be careful and show you exactly what the next step is. If you exponentiate this fact, you take e to this power, that's going to tend to e to the first power, which is just e, all right? And then we just observe that this is the same as ak, right? So the basic ingredient here is that e to the log a is equal to a. That's because the log function is the inverse of the exponential function. Yes, question? Zero. So, tending to one. Who said it was? k times, if you take the logarithm, which is what we did last time. Logarithm of a k is indeed k times the log\n"
     ]
    }
   ],
   "source": [
    "output = pipe_l(\"output.wav\")[\"text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was one, as k goes to infinity. Now, so last time this is what we did, and I just wanted to be careful and show you exactly what the next step is. If you exponentiate this fact, you take e to this power, that's gonna tend to e to the first power, which is just e, all right? And then we just observe that this is the same as ak, right? So the basic ingredient here is that e to the log a is equal to a. That's because the log function is the inverse of the exponential function. Yes, question. Zero. So 10 to 1. Who said it was? k times, if you take the logarithm, which is what we did last time, The logarithm of ak is indeed k times the log.\n"
     ]
    }
   ],
   "source": [
    "output = pipe_m(\"output.wav\")[\"text\"]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was one, as k goes to infinity. Now, so last time this is what we did and I just wanted to be careful and show you exactly what the next step is. If you exponentiate this fact, you take e to this power, that's going to tend to e to the first power, which is just e. Alright? And then we just observe that this is is the same as ak, right? So the basic ingredient here is that the log a is equal to a. That's because the log function is the inverse of the exponential function. Yes, question. Zero. So, tending to one. Who said it was? k times, if you take the logarithm, which is what we did last time. Logarithm of ak is indeed k times the log.\n"
     ]
    }
   ],
   "source": [
    "output = pipe_s(\"output.wav\")[\"text\"]\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
